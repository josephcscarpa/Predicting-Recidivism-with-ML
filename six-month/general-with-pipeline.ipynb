{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Recidivism with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Import data\"\n",
    "# Whole data set for data-exploration\n",
    "whole_dataframe = pd.read_csv(\"../data/broward_data.csv\")\n",
    "# Test train split used in XYZ paper for training our models. \n",
    "train_dataframe = pd.read_csv(\"../data/broward_train.csv\")\n",
    "test_dataframe = pd.read_csv(\"../data/broward_test.csv\")\n",
    "\n",
    "# whole_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Removing person_id, screening_date because they are not helpful to our analyses\n",
    "Removing current_violence20 because redundant with 2 other columns. \n",
    "\"\"\"\n",
    "whole_dataframe = whole_dataframe.drop(['person_id', \"screening_date\", \"current_violence20\"], axis=1)\n",
    "test_dataframe = test_dataframe.drop(['person_id', \"screening_date\", \"current_violence20\"], axis=1)\n",
    "train_dataframe = train_dataframe.drop(['person_id', \"screening_date\", \"current_violence20\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_mapping = {\"African-American\" : 0, \"Caucasian\" : 1, \"Hispanic\" : 2, \"Other\" : 3, \"Asian\" : 4, \"Native American\" : 5, }\n",
    "whole_dataframe['race'] = whole_dataframe['race'].map(race_mapping)\n",
    "train_dataframe['race'] = train_dataframe['race'].map(race_mapping)\n",
    "test_dataframe['race'] = test_dataframe['race'].map(race_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column_names = ['six_month', 'one_year', 'three_year', 'five_year', 'general_two_year',\n",
    "       'general_six_month', 'drug_two_year', 'property_two_year',\n",
    "       'misdemeanor_two_year', 'felony_two_year', 'violent_two_year',\n",
    "       'drug_six_month', 'property_six_month', 'misdemeanor_six_month',\n",
    "       'felony_six_month', 'violent_six_month']\n",
    "\n",
    "whole_dataframe_X = whole_dataframe.drop(label_column_names, axis = 1)\n",
    "whole_dataframe_label_choices = whole_dataframe[label_column_names]\n",
    "whole_data_X = whole_dataframe_X.values\n",
    "\"To get numpy y-labels: append {.astype(int).values} to end of label series\"\n",
    "\n",
    "test_dataframe_X = test_dataframe.drop(label_column_names, axis = 1)\n",
    "test_dataframe_label_choices = test_dataframe[label_column_names]\n",
    "test_data_X = test_dataframe_X.values\n",
    "test_data_y = test_dataframe_label_choices[\"general_six_month\"].astype(int).values\n",
    "\n",
    "train_dataframe_X = train_dataframe.drop(label_column_names, axis = 1)\n",
    "train_dataframe_label_choices = train_dataframe[label_column_names]\n",
    "train_data_X = train_dataframe_X.values\n",
    "train_data_y = train_dataframe_label_choices[\"general_six_month\"].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data_X\n",
    "y_train = train_data_y\n",
    "X_test = test_data_X\n",
    "y_test = test_data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(671)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Metrics\"\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics: AUC\n",
    "Additionally, our evaluation metric is AUC, which is a rank statistic, and considers relative risk rather than absolute risk;\n",
    "\n",
    "https://datascience.stackexchange.com/questions/30881/when-is-precision-more-important-over-recall#:~:text=Thus%2C%20precision%20will%20be%20more,having%20any%20action%20at%20all%22.\n",
    "\n",
    "Metric: Precision\n",
    "Picked this metric on the assumption that a false negative (predicting a person will not be charged when they will be) is better than false positives.\n",
    "\n",
    "Metric: F1-Score:\n",
    "A good metric because our dataset is imbalanced and combines the power of precision and recall. \n",
    "\n",
    "Metric: Log-Loss:\n",
    "Useful for assessing the uncertainty of our predictions, which is useful, considering the decisions of our decisions affect real people. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg_grid = {\n",
    "    'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'fit_intercept': [True, False],\n",
    "    'C':[10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 10**0, 10**2],\n",
    "    'tol': [.00001, .0001, .001, .01, .1, 1],\n",
    "    'max_iter': [100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "lreg_cv = GridSearchCV(estimator = LogisticRegression(), \n",
    "                                    param_grid = lreg_grid, \n",
    "                                    cv = 5\n",
    "                                    )\n",
    "lreg_cv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.0001, 'fit_intercept': True, 'max_iter': 1000, 'penalty': 'l2', 'tol': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "print(lreg_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.0001, tol=1e-05)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.0001, tol=1e-05)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.0001, tol=1e-05)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_clf = LogisticRegression(\n",
    "    penalty = lreg_cv.best_params_.get('penalty'), \n",
    "    fit_intercept = lreg_cv.best_params_.get('fit_intercept'), \n",
    "    C = lreg_cv.best_params_.get('C'),\n",
    "    tol = lreg_cv.best_params_.get('tol')\n",
    "    )\n",
    "lreg_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Logistic Regression for Best Model\n",
      "0.6104918032786886\n",
      "[[0.78206778 0.21793222]\n",
      " [0.80959897 0.19040103]\n",
      " [0.63900427 0.36099573]\n",
      " [0.80792597 0.19207403]\n",
      " [0.84456221 0.15543779]\n",
      " [0.83733133 0.16266867]\n",
      " [0.75110304 0.24889696]\n",
      " [0.82686496 0.17313504]\n",
      " [0.8033964  0.1966036 ]\n",
      " [0.6997942  0.3002058 ]\n",
      " [0.76308283 0.23691717]\n",
      " [0.8857704  0.1142296 ]\n",
      " [0.7354156  0.2645844 ]\n",
      " [0.74916864 0.25083136]\n",
      " [0.74100552 0.25899448]\n",
      " [0.83867543 0.16132457]\n",
      " [0.79843237 0.20156763]\n",
      " [0.73753309 0.26246691]\n",
      " [0.88239434 0.11760566]\n",
      " [0.77313252 0.22686748]\n",
      " [0.81093355 0.18906645]\n",
      " [0.75396693 0.24603307]\n",
      " [0.77622516 0.22377484]\n",
      " [0.83816498 0.16183502]\n",
      " [0.74684489 0.25315511]\n",
      " [0.70900488 0.29099512]\n",
      " [0.84701401 0.15298599]\n",
      " [0.71920554 0.28079446]\n",
      " [0.7448174  0.2551826 ]\n",
      " [0.80457671 0.19542329]\n",
      " [0.71221409 0.28778591]\n",
      " [0.84933504 0.15066496]\n",
      " [0.75545407 0.24454593]\n",
      " [0.7648677  0.2351323 ]\n",
      " [0.8633039  0.1366961 ]\n",
      " [0.76658117 0.23341883]\n",
      " [0.73810966 0.26189034]\n",
      " [0.66647553 0.33352447]\n",
      " [0.7694448  0.2305552 ]\n",
      " [0.86541789 0.13458211]\n",
      " [0.68317794 0.31682206]\n",
      " [0.8379429  0.1620571 ]\n",
      " [0.77059926 0.22940074]\n",
      " [0.78548904 0.21451096]\n",
      " [0.6716915  0.3283085 ]\n",
      " [0.67196879 0.32803121]\n",
      " [0.76552549 0.23447451]\n",
      " [0.79244283 0.20755717]\n",
      " [0.80374457 0.19625543]\n",
      " [0.84335439 0.15664561]\n",
      " [0.76684422 0.23315578]\n",
      " [0.85839283 0.14160717]\n",
      " [0.75568341 0.24431659]\n",
      " [0.85687911 0.14312089]\n",
      " [0.8129436  0.1870564 ]\n",
      " [0.87573115 0.12426885]\n",
      " [0.80544154 0.19455846]\n",
      " [0.66470402 0.33529598]\n",
      " [0.80724637 0.19275363]\n",
      " [0.75046072 0.24953928]\n",
      " [0.85168616 0.14831384]\n",
      " [0.830738   0.169262  ]\n",
      " [0.86012043 0.13987957]\n",
      " [0.75993656 0.24006344]\n",
      " [0.80152292 0.19847708]\n",
      " [0.79397397 0.20602603]\n",
      " [0.72294759 0.27705241]\n",
      " [0.81553345 0.18446655]\n",
      " [0.77066117 0.22933883]\n",
      " [0.7916783  0.2083217 ]\n",
      " [0.7661689  0.2338311 ]\n",
      " [0.85683641 0.14316359]\n",
      " [0.86804918 0.13195082]\n",
      " [0.83195435 0.16804565]\n",
      " [0.78174331 0.21825669]\n",
      " [0.83978501 0.16021499]\n",
      " [0.84244411 0.15755589]\n",
      " [0.76105096 0.23894904]\n",
      " [0.80566112 0.19433888]\n",
      " [0.75277622 0.24722378]\n",
      " [0.84941407 0.15058593]\n",
      " [0.73806311 0.26193689]\n",
      " [0.75607728 0.24392272]\n",
      " [0.6792887  0.3207113 ]\n",
      " [0.76968797 0.23031203]\n",
      " [0.79151482 0.20848518]\n",
      " [0.72867303 0.27132697]\n",
      " [0.75835099 0.24164901]\n",
      " [0.77066587 0.22933413]\n",
      " [0.67619379 0.32380621]\n",
      " [0.81796041 0.18203959]\n",
      " [0.80089461 0.19910539]\n",
      " [0.77816967 0.22183033]\n",
      " [0.79509714 0.20490286]\n",
      " [0.7483022  0.2516978 ]\n",
      " [0.78399704 0.21600296]\n",
      " [0.73971428 0.26028572]\n",
      " [0.79134954 0.20865046]\n",
      " [0.7642512  0.2357488 ]\n",
      " [0.78515238 0.21484762]\n",
      " [0.78788934 0.21211066]\n",
      " [0.76684989 0.23315011]\n",
      " [0.87443669 0.12556331]\n",
      " [0.8673556  0.1326444 ]\n",
      " [0.71447103 0.28552897]\n",
      " [0.83301996 0.16698004]\n",
      " [0.72071399 0.27928601]\n",
      " [0.81429482 0.18570518]\n",
      " [0.70228172 0.29771828]\n",
      " [0.79748118 0.20251882]\n",
      " [0.75413547 0.24586453]\n",
      " [0.78453424 0.21546576]\n",
      " [0.66028619 0.33971381]\n",
      " [0.75476662 0.24523338]\n",
      " [0.76546295 0.23453705]\n",
      " [0.75112329 0.24887671]\n",
      " [0.72237238 0.27762762]\n",
      " [0.79432582 0.20567418]\n",
      " [0.79919808 0.20080192]\n",
      " [0.83481478 0.16518522]\n",
      " [0.72261754 0.27738246]\n",
      " [0.87955247 0.12044753]\n",
      " [0.79859368 0.20140632]\n",
      " [0.75819008 0.24180992]\n",
      " [0.76932087 0.23067913]\n",
      " [0.77008023 0.22991977]\n",
      " [0.77822304 0.22177696]\n",
      " [0.76988125 0.23011875]\n",
      " [0.85170789 0.14829211]\n",
      " [0.79542961 0.20457039]\n",
      " [0.84466699 0.15533301]\n",
      " [0.82681102 0.17318898]\n",
      " [0.72688773 0.27311227]\n",
      " [0.84801999 0.15198001]\n",
      " [0.79452627 0.20547373]\n",
      " [0.8837687  0.1162313 ]\n",
      " [0.75388209 0.24611791]\n",
      " [0.84766595 0.15233405]\n",
      " [0.83145959 0.16854041]\n",
      " [0.74456421 0.25543579]\n",
      " [0.75475459 0.24524541]\n",
      " [0.76655538 0.23344462]\n",
      " [0.79610149 0.20389851]\n",
      " [0.78813764 0.21186236]\n",
      " [0.66618369 0.33381631]\n",
      " [0.72686825 0.27313175]\n",
      " [0.58325582 0.41674418]\n",
      " [0.71856828 0.28143172]\n",
      " [0.86484211 0.13515789]\n",
      " [0.73633041 0.26366959]\n",
      " [0.70223798 0.29776202]\n",
      " [0.63885762 0.36114238]\n",
      " [0.81236773 0.18763227]\n",
      " [0.58858163 0.41141837]\n",
      " [0.67358684 0.32641316]\n",
      " [0.79581662 0.20418338]\n",
      " [0.75576006 0.24423994]\n",
      " [0.72525351 0.27474649]\n",
      " [0.82341626 0.17658374]\n",
      " [0.71189523 0.28810477]\n",
      " [0.76516393 0.23483607]\n",
      " [0.75086377 0.24913623]\n",
      " [0.77678698 0.22321302]\n",
      " [0.71833677 0.28166323]\n",
      " [0.75809384 0.24190616]\n",
      " [0.75453512 0.24546488]\n",
      " [0.70533999 0.29466001]\n",
      " [0.75760285 0.24239715]\n",
      " [0.75243763 0.24756237]\n",
      " [0.77163308 0.22836692]\n",
      " [0.76431603 0.23568397]\n",
      " [0.63746178 0.36253822]\n",
      " [0.77922879 0.22077121]\n",
      " [0.78205868 0.21794132]\n",
      " [0.77239384 0.22760616]\n",
      " [0.78061304 0.21938696]\n",
      " [0.79744848 0.20255152]\n",
      " [0.67724625 0.32275375]\n",
      " [0.83518714 0.16481286]\n",
      " [0.70426768 0.29573232]\n",
      " [0.7875653  0.2124347 ]\n",
      " [0.74843883 0.25156117]\n",
      " [0.80343743 0.19656257]\n",
      " [0.84940238 0.15059762]\n",
      " [0.69783206 0.30216794]\n",
      " [0.74282684 0.25717316]\n",
      " [0.59349985 0.40650015]\n",
      " [0.7796374  0.2203626 ]\n",
      " [0.795283   0.204717  ]\n",
      " [0.77571255 0.22428745]\n",
      " [0.67944489 0.32055511]\n",
      " [0.77286723 0.22713277]\n",
      " [0.80795544 0.19204456]\n",
      " [0.79964593 0.20035407]\n",
      " [0.82088746 0.17911254]\n",
      " [0.76180829 0.23819171]\n",
      " [0.82323551 0.17676449]\n",
      " [0.83646347 0.16353653]\n",
      " [0.76782916 0.23217084]\n",
      " [0.75430574 0.24569426]\n",
      " [0.76988279 0.23011721]\n",
      " [0.77215387 0.22784613]\n",
      " [0.76506608 0.23493392]\n",
      " [0.7365202  0.2634798 ]\n",
      " [0.69809751 0.30190249]\n",
      " [0.82237629 0.17762371]\n",
      " [0.80971636 0.19028364]\n",
      " [0.85972503 0.14027497]\n",
      " [0.78201615 0.21798385]\n",
      " [0.77864177 0.22135823]\n",
      " [0.80780456 0.19219544]\n",
      " [0.83116165 0.16883835]\n",
      " [0.76584207 0.23415793]\n",
      " [0.78702999 0.21297001]\n",
      " [0.70388627 0.29611373]\n",
      " [0.76032462 0.23967538]\n",
      " [0.77984251 0.22015749]\n",
      " [0.79759681 0.20240319]\n",
      " [0.7615581  0.2384419 ]\n",
      " [0.77725473 0.22274527]\n",
      " [0.69533553 0.30466447]\n",
      " [0.74165641 0.25834359]\n",
      " [0.8206103  0.1793897 ]\n",
      " [0.80873446 0.19126554]\n",
      " [0.76889457 0.23110543]\n",
      " [0.73914496 0.26085504]\n",
      " [0.78866541 0.21133459]\n",
      " [0.83034769 0.16965231]\n",
      " [0.82716579 0.17283421]\n",
      " [0.74266469 0.25733531]\n",
      " [0.7594361  0.2405639 ]\n",
      " [0.76294407 0.23705593]\n",
      " [0.79946304 0.20053696]\n",
      " [0.85764499 0.14235501]\n",
      " [0.86473854 0.13526146]\n",
      " [0.87257359 0.12742641]\n",
      " [0.74672444 0.25327556]\n",
      " [0.73806454 0.26193546]\n",
      " [0.79637941 0.20362059]\n",
      " [0.75229425 0.24770575]\n",
      " [0.85027176 0.14972824]\n",
      " [0.74614356 0.25385644]\n",
      " [0.73472758 0.26527242]\n",
      " [0.75441559 0.24558441]\n",
      " [0.68350115 0.31649885]\n",
      " [0.75589967 0.24410033]\n",
      " [0.74053221 0.25946779]\n",
      " [0.77399697 0.22600303]\n",
      " [0.81262868 0.18737132]\n",
      " [0.81824919 0.18175081]\n",
      " [0.71410139 0.28589861]\n",
      " [0.64844659 0.35155341]\n",
      " [0.67952028 0.32047972]\n",
      " [0.61638255 0.38361745]\n",
      " [0.75919082 0.24080918]\n",
      " [0.78834738 0.21165262]\n",
      " [0.84036455 0.15963545]\n",
      " [0.83207233 0.16792767]\n",
      " [0.75889427 0.24110573]\n",
      " [0.75399987 0.24600013]\n",
      " [0.72057458 0.27942542]\n",
      " [0.73288558 0.26711442]\n",
      " [0.7714699  0.2285301 ]\n",
      " [0.78600472 0.21399528]\n",
      " [0.90084026 0.09915974]\n",
      " [0.75877638 0.24122362]\n",
      " [0.72113314 0.27886686]\n",
      " [0.80611228 0.19388772]\n",
      " [0.82757056 0.17242944]\n",
      " [0.75543805 0.24456195]\n",
      " [0.76379906 0.23620094]\n",
      " [0.76489911 0.23510089]\n",
      " [0.84193698 0.15806302]\n",
      " [0.77558845 0.22441155]\n",
      " [0.73800887 0.26199113]\n",
      " [0.47842802 0.52157198]\n",
      " [0.8102406  0.1897594 ]\n",
      " [0.75949876 0.24050124]\n",
      " [0.80179234 0.19820766]\n",
      " [0.77551444 0.22448556]\n",
      " [0.85407498 0.14592502]\n",
      " [0.7779798  0.2220202 ]\n",
      " [0.82837654 0.17162346]\n",
      " [0.76770085 0.23229915]\n",
      " [0.74621222 0.25378778]\n",
      " [0.73866535 0.26133465]\n",
      " [0.79688551 0.20311449]\n",
      " [0.82263979 0.17736021]\n",
      " [0.78444384 0.21555616]\n",
      " [0.84299822 0.15700178]\n",
      " [0.76357343 0.23642657]\n",
      " [0.79197878 0.20802122]\n",
      " [0.75896137 0.24103863]\n",
      " [0.74769886 0.25230114]\n",
      " [0.69608722 0.30391278]\n",
      " [0.8787428  0.1212572 ]\n",
      " [0.61704806 0.38295194]\n",
      " [0.74724024 0.25275976]\n",
      " [0.787672   0.212328  ]\n",
      " [0.78211809 0.21788191]\n",
      " [0.68095601 0.31904399]\n",
      " [0.7990411  0.2009589 ]\n",
      " [0.83194029 0.16805971]\n",
      " [0.76405062 0.23594938]\n",
      " [0.74031587 0.25968413]\n",
      " [0.8164952  0.1835048 ]\n",
      " [0.76691539 0.23308461]\n",
      " [0.87827254 0.12172746]\n",
      " [0.7335801  0.2664199 ]\n",
      " [0.77123293 0.22876707]\n",
      " [0.7754476  0.2245524 ]\n",
      " [0.77166204 0.22833796]\n",
      " [0.75726181 0.24273819]\n",
      " [0.77244329 0.22755671]\n",
      " [0.70690021 0.29309979]\n",
      " [0.76057677 0.23942323]\n",
      " [0.86239568 0.13760432]\n",
      " [0.77245923 0.22754077]\n",
      " [0.78418421 0.21581579]\n",
      " [0.71440489 0.28559511]\n",
      " [0.70935745 0.29064255]\n",
      " [0.81632011 0.18367989]\n",
      " [0.73913495 0.26086505]\n",
      " [0.81791761 0.18208239]\n",
      " [0.77404722 0.22595278]\n",
      " [0.78522533 0.21477467]\n",
      " [0.6982632  0.3017368 ]\n",
      " [0.79351867 0.20648133]\n",
      " [0.76664141 0.23335859]\n",
      " [0.75139049 0.24860951]\n",
      " [0.77417012 0.22582988]\n",
      " [0.79793351 0.20206649]\n",
      " [0.82700348 0.17299652]\n",
      " [0.68747527 0.31252473]\n",
      " [0.79182333 0.20817667]\n",
      " [0.69504241 0.30495759]\n",
      " [0.8126979  0.1873021 ]\n",
      " [0.86484381 0.13515619]\n",
      " [0.82102956 0.17897044]\n",
      " [0.84608863 0.15391137]\n",
      " [0.77125193 0.22874807]\n",
      " [0.86517834 0.13482166]\n",
      " [0.76439362 0.23560638]\n",
      " [0.84081423 0.15918577]\n",
      " [0.71263439 0.28736561]\n",
      " [0.79771581 0.20228419]\n",
      " [0.8755889  0.1244111 ]\n",
      " [0.76083538 0.23916462]\n",
      " [0.79264489 0.20735511]\n",
      " [0.89138574 0.10861426]\n",
      " [0.81689008 0.18310992]\n",
      " [0.80911297 0.19088703]\n",
      " [0.7891004  0.2108996 ]\n",
      " [0.89188135 0.10811865]\n",
      " [0.86060787 0.13939213]\n",
      " [0.84898289 0.15101711]\n",
      " [0.80574903 0.19425097]\n",
      " [0.68934128 0.31065872]\n",
      " [0.80795222 0.19204778]\n",
      " [0.75815564 0.24184436]\n",
      " [0.79595751 0.20404249]\n",
      " [0.79248943 0.20751057]\n",
      " [0.78707952 0.21292048]\n",
      " [0.81678201 0.18321799]\n",
      " [0.72994513 0.27005487]\n",
      " [0.79692247 0.20307753]\n",
      " [0.72354716 0.27645284]\n",
      " [0.76872815 0.23127185]\n",
      " [0.73186853 0.26813147]\n",
      " [0.87533504 0.12466496]\n",
      " [0.76512203 0.23487797]\n",
      " [0.81049841 0.18950159]\n",
      " [0.81614772 0.18385228]\n",
      " [0.8540429  0.1459571 ]\n",
      " [0.87536635 0.12463365]\n",
      " [0.82009265 0.17990735]\n",
      " [0.75943742 0.24056258]\n",
      " [0.71165038 0.28834962]\n",
      " [0.79127429 0.20872571]\n",
      " [0.75851994 0.24148006]\n",
      " [0.82962137 0.17037863]\n",
      " [0.82949526 0.17050474]\n",
      " [0.70931606 0.29068394]\n",
      " [0.79227894 0.20772106]\n",
      " [0.61471411 0.38528589]\n",
      " [0.78122824 0.21877176]\n",
      " [0.6959841  0.3040159 ]\n",
      " [0.66064463 0.33935537]\n",
      " [0.75593922 0.24406078]\n",
      " [0.63937372 0.36062628]]\n",
      "Log Loss of Logistic Regression for Best Model\n",
      "7.4454016965471475\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC of Logistic Regression for Best Model\")\n",
    "print(roc_auc_score(y_test, lreg_clf.decision_function(X_test)))\n",
    "\n",
    "# Convert to binary predictions\n",
    "y_pred_prob_to_binary = lreg_clf.predict_proba(X_test)\n",
    "print(y_pred_prob_to_binary)\n",
    "\n",
    "# Used only for model comparion\n",
    "print(\"Log Loss of Logistic Regression for Best Model\")\n",
    "print(log_loss(y_test, lreg_clf.decision_function(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_grid = {\n",
    "    'alpha': [2, 4, 8, 16, 32, 64, 128],\n",
    "    'fit_intercept': [True, False],\n",
    "    'tol': [.00001, .0001, .001, .01, .1, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv = GridSearchCV(estimator = Lasso(), \n",
    "                                    param_grid = lasso_grid, \n",
    "                                    cv = 5\n",
    "                                    )\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "print(lasso_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_clf = Lasso(\n",
    "    alpha = lasso_cv.best_params_.get('alpha'), \n",
    "    fit_intercept = lasso_cv.best_params_.get('fit_intercept'), \n",
    "    tol = lasso_cv.best_params_.get('tol'), \n",
    "    )\n",
    "lasso_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Fold Validation for Random Forest\n",
    "\n",
    "rf_grid = {\n",
    "    'n_estimators': [2, 4, 8, 16, 32, 64, 128],\n",
    "    'max_features': [1, 2, 3, 4, 5, 6],\n",
    "    'max_depth': list(range(1, 32+1)),\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'], \n",
    "    'random_state' : [0]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv = GridSearchCV(estimator = RandomForestClassifier(), \n",
    "                                    param_grid = rf_grid, \n",
    "                                    cv = 5\n",
    "                                    )\n",
    "rf_cv.fit(X_train, y_train)\n",
    "print(rf_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators = rf_cv.best_params_.get('n_estimators'), \n",
    "    max_depth = rf_cv.best_params_.get('max_depth'), \n",
    "    max_features = rf_cv.best_params_.get('max_features'), \n",
    "    criterion = rf_cv.best_params_.get('criterion')\n",
    "    )\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosted Decision Trees (ADA Boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_grid = {\n",
    "    'n_estimators': [2, 4, 8, 16, 32, 64, 128],\n",
    "    'learning_rate': [0.1, 0.25, 0.5, 0.75, 0.9, 1.0, 1.25, 1.5, 2],\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "    'random_state' : [0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_cv = GridSearchCV(\n",
    "    estimator = AdaBoostClassifier(),\n",
    "    param_grid = ada_grid,\n",
    "    cv = 5\n",
    ")\n",
    "ada_cv.fit(X_train, y_train)\n",
    "print(ada_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf= AdaBoostClassifier(\n",
    "    n_estimators = ada_cv.best_params_.get('n_estimators'), \n",
    "    learning_rate = ada_cv.best_params_.get('learning_rate'), \n",
    "    algorithm = ada_cv.best_params_.get('algorithm')\n",
    "    )\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a25f99aa125bd4410be9155a2fb8511cd1ed013e1077370425d4be963778dfe3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
